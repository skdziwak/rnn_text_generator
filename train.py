#
# RNN TEXT GENERATOR v1.0
#
# by Szymon Dziwak
# https://github.com/skdziwak/rnn_text_generator
#

import numpy as np
import os, json, argparse

parser = argparse.ArgumentParser()
parser.add_argument('--epochs', '-ep', dest='epochs', action='store', type=int, default=10000, help='Epochs (default: 10000)')
parser.add_argument('--embedding-dimension', '-ed', dest='embedding_dimension', action='store', type=int, default=256, help='Embedding layer output dimension (default: 256)')
parser.add_argument('--rnn-units', '-ru', dest='rnn_units', action='store', type=int, default=1024, help='GRU layer units (default: 1024)')
parser.add_argument('--buffer-size', dest='buffer_size', action='store', type=int, default=1000, help='Dataset shuffling buffer size (default: 1000)')
parser.add_argument('--batch-size', dest='batch_size', action='store', type=int, default=32, help='Training batch size (default: 32)')
parser.add_argument('--sequence-length', '-sl', dest='sequence_length', action='store', type=int, default=150, help='Training batch size (default: 150)')
parser.add_argument('--info', '-i', dest='info', action='store', type=str, default='info.json', help='Path where info file will be saved (default: info.json)')
parser.add_argument('--early-stopping', '-es', dest='early_stopping', action='store', type=int, default=0, help='Early stopping patience (default: 0, no early stopping)')
parser.add_argument('--training-path', '-tp', dest='training_path', action='store', type=str, default='training.txt', help='Path to training file or directory of training files (default: training.txt)')
parser.add_argument('--checkpoints-path', '-cp', dest='checkpoints_path', action='store', type=str, default='checkpoints', help='Path to directory where weights will be saved (default: checkpoints)')
args, remaining = parser.parse_known_args()

from tensorflow import keras
import tensorflow as tf

for gpu in tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(gpu, True)

# Load input text for RNN
if os.path.isdir(args.training_path):
    text = ''
    for name in os.listdir(args.training_path):
        with open(os.path.join(args.training_path, name), 'r', encoding='utf-8') as file:
            text += file.read() + ' '
else:
    with open(args.training_path, 'r', encoding='utf-8') as file:
        text=file.read() + ' '

# Prepare vocabulary
vocab = sorted(set(text))
vocab_size = len(vocab)
char2id = {c: i for i, c in enumerate(vocab)}
id2char = np.array(vocab)

# Encode text
encoded_text = [char2id[c] for c in text]

# Create two shifted arrays from encoded_text
inputs = np.array(encoded_text[:-1])
targets = np.array(encoded_text[1:])

# Prepare dataset
dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))
dataset = dataset.batch(args.sequence_length, drop_remainder=True) # Split text into sequences that have equal length
dataset = dataset.shuffle(args.buffer_size).batch(args.batch_size, drop_remainder=True) # Shuffle sequences and then prepare batches

# Create info file for scripts using neural network generated by this script
with open(args.info, 'w', encoding='utf-8') as file:
    file.write(json.dumps({
        'vocabulary': vocab,
        'embedding_dimension': args.embedding_dimension,
        'rnn_units': args.rnn_units,
        'checkpoints_path': args.checkpoints_path
    }))


# Create neural network based on input
model = keras.Sequential([
    keras.layers.Embedding(vocab_size, args.embedding_dimension, batch_input_shape=[args.batch_size, None]),
    keras.layers.GRU(args.rnn_units, stateful=True, return_sequences=True, recurrent_initializer='glorot_uniform'),
    keras.layers.Dense(vocab_size)
])
def loss(labels, logits):
    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

model.compile(optimizer='adam', loss=loss)
model.summary()

# Prepare callbacks
callbacks =[
    keras.callbacks.ModelCheckpoint(os.path.join(args.checkpoints_path, '{epoch}'), save_weights_only=True) # Select the directory where progress will be saved
]
if args.early_stopping > 0:
    callbacks.append(keras.callbacks.EarlyStopping(monitor=['val_loss'], patience=args.early_stopping))

# Start training
model.fit(dataset, epochs=args.epochs, callbacks=callbacks)
